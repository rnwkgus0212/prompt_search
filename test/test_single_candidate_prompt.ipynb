{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3922981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jahyun/.conda/envs/reinforcement/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /home/jahyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jahyun/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst-2\n",
      "test\n",
      "load model...\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor, AgnewsProcessor\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import os, sys\n",
    "\n",
    "import random\n",
    "\n",
    "# Set a random seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "\n",
    "\n",
    "wordnet.synsets\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# experiment configs!\n",
    "\n",
    "\n",
    "dataset_name = \"sst-2\"\n",
    "split = \"test\"\n",
    "\n",
    "model = \"roberta\"\n",
    "model_config = \"roberta-large\"\n",
    "\n",
    "iteration = 5\n",
    "\n",
    "inference_count = 0\n",
    "\n",
    "count_from_highest = 15\n",
    "# shots = 200\n",
    "# shots = 64\n",
    "shots = 32\n",
    "\n",
    "proportional_decrease=0.5\n",
    "proportional_increase=2\n",
    "\n",
    "project = f\"project_{count_from_highest}_{shots}_{seed}\"\n",
    "\n",
    "if_testing_small_run = None # should be changed to None\n",
    "if if_testing_small_run:\n",
    "    project += '_testrun'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(dataset_name)\n",
    "print(split)\n",
    "\n",
    "dataset = {}\n",
    "# dataset['train'] = SST2Processor().get_train_examples(\"./datasets/TextClassification/SST-2\")\n",
    "# dataset['validation'] = SST2Processor().get_dev_examples(\"./datasets/TextClassification/SST-2\")\n",
    "\n",
    "\n",
    "print('load model...')\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(model, model_config)\n",
    "print(\"model loaded\")\n",
    "\n",
    "\n",
    "# get specific some info from dataset\n",
    "\n",
    "full_data_len = 0\n",
    "\n",
    "if dataset_name == \"sst-2\":\n",
    "    temp = SST2Processor().get_test_examples(\"./datasets/TextClassification/SST-2\")\n",
    "    full_data_len = len(temp)\n",
    "\n",
    "\n",
    "classes = [ \n",
    "    \"negative\",\n",
    "    \"positive\"\n",
    "]\n",
    "\n",
    "label_words = {\n",
    "        \"negative\": [\"terrible\"],\n",
    "        \"positive\": [\"great\"],\n",
    "    }\n",
    "\n",
    "dataset['test'] = SST2Processor().get_test_examples(\"./datasets/TextClassification/SST-2\")\n",
    "dataset['test'] = random.sample(dataset['test'], full_data_len)\n",
    "\n",
    "def get_score(dataset_name,dataset,shots,candidate):\n",
    "\n",
    "\n",
    "    if dataset_name == \"sst-2\":\n",
    "\n",
    "        \n",
    "        \n",
    "        if shots:\n",
    "            temp_dataset = dataset['test'][:shots]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        the_prompt_to_be_found = candidate\n",
    "        \n",
    "        template = '{\"placeholder\": \"text_a\"} ' + the_prompt_to_be_found  + ' {\"mask\"}.'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    promptTemplate = ManualTemplate(\n",
    "        text = template,\n",
    "        tokenizer = tokenizer,\n",
    "    )\n",
    "    promptVerbalizer = ManualVerbalizer(\n",
    "        classes = classes,\n",
    "        label_words = label_words,\n",
    "        tokenizer = tokenizer,\n",
    "    )\n",
    "    promptModel = PromptForClassification(\n",
    "        template = promptTemplate,\n",
    "        plm = plm,\n",
    "        verbalizer = promptVerbalizer,\n",
    "    )\n",
    "    promptModel=  promptModel.cuda()\n",
    "    data_loader = PromptDataLoader(\n",
    "        dataset = temp_dataset,\n",
    "        tokenizer = tokenizer,\n",
    "        template = promptTemplate,\n",
    "        tokenizer_wrapper_class=WrapperClass,\n",
    "    )\n",
    "    promptModel.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    \n",
    "    # scorer is evaluation\n",
    "\n",
    "    for step, inputs in tqdm(enumerate(data_loader)):\n",
    "\n",
    "        inputs = inputs.cuda()\n",
    "        logits = promptModel(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "\n",
    "    result = f\"{the_prompt_to_be_found.strip().ljust(150)}{str(round(acc,4)).ljust(10)}{str(shots).ljust(10)}\\n\"\n",
    "    print(result)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a5fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"It was very\",\n",
    "    \"Absolutely\",\n",
    "    \"AbsolutelyAbsolutely\",\n",
    "    \"Really downright\",\n",
    "    \"Absolutely VERY absolute VERY absolute\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7425911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb5fa07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1407.60it/s]\n",
      "1821it [01:11, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was very                                                                                                                                           0.8396    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1506.94it/s]\n",
      "1821it [01:12, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely                                                                                                                                            0.916     1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1820.54it/s]\n",
      "1821it [01:12, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbsolutelyAbsolutely                                                                                                                                  0.9006    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1784.61it/s]\n",
      "1821it [01:12, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Really downright                                                                                                                                      0.9154    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1656.18it/s]\n",
      "1821it [01:13, 24.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely VERY absolute VERY absolute                                                                                                                0.9325    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e21af5",
   "metadata": {},
   "source": [
    "It was very                                                                                                                                           \n",
    "0.8396    1821      \n",
    "\n",
    "tokenizing: 1821it [00:01, 1506.94it/s]\n",
    "1821it [01:12, 25.15it/s]\n",
    "Absolutely                                                                                                                                            \n",
    "0.916     1821      \n",
    "\n",
    "tokenizing: 1821it [00:01, 1820.54it/s]\n",
    "1821it [01:12, 25.01it/s]\n",
    "AbsolutelyAbsolutely                                                                                                                                  \n",
    "0.9006    1821      \n",
    "\n",
    "tokenizing: 1821it [00:01, 1784.61it/s]\n",
    "1821it [01:12, 24.96it/s]\n",
    "Really downright                                                                                                                                      \n",
    "0.9154    1821      \n",
    "\n",
    "tokenizing: 1821it [00:01, 1656.18it/s]\n",
    "1821it [01:13, 24.86it/s]\n",
    "Absolutely VERY absolute VERY absolute                                                                                                                \n",
    "0.9325    1821      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff37982",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"ĠwidelyĠcriticallyĠpassionatelyĠfirmlyĠwidely\",\n",
    "    \"ĠstrategicallyĠfirmly\",\n",
    "    \"ĠwidelyĠcritically ĠpassionatelyĠfirmly\"\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d702ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1673.31it/s]\n",
      "1821it [01:12, 25.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠwidelyĠcriticallyĠpassionatelyĠfirmlyĠwidely                                                                                                         0.9099    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1713.67it/s]\n",
      "1821it [01:13, 24.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trategicallyĠfirmly                                                                                                                                   0.883     1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1706.55it/s]\n",
      "1821it [01:12, 24.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "widelyĠcritically ĠpassionatelyĠfirmly                                                                                                                0.8462    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60bb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"ĠwidelyĠcriticallyĠpassionatelyĠfirmlyĠwidely\",\n",
    "    \"ĠstrategicallyĠfirmly\",\n",
    "    \"ĠwidelyĠcritically ĠpassionatelyĠfirmly\"\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4be9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1638.26it/s]\n",
      "1821it [01:12, 25.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠwidelyĠcriticallyĠpassionatelyĠfirmlyĠwidely                                                                                                         0.9099    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1772.75it/s]\n",
      "1821it [01:12, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠstrategicallyĠfirmly                                                                                                                                 0.8913    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1684.90it/s]\n",
      "1821it [01:13, 24.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠwidelyĠcritically ĠpassionatelyĠfirmly                                                                                                               0.9083    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2114075",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"widelycriticallypassionatelyfirmlywidely\",\n",
    "    \"strategicallyfirmly\",\n",
    "    \"widelycritically passionatelyfirmly\"\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61b5b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1745.82it/s]\n",
      "1821it [01:10, 25.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "widelycriticallypassionatelyfirmlywidely                                                                                                              0.9039    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:00, 1835.48it/s]\n",
      "1821it [01:12, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategicallyfirmly                                                                                                                                   0.8232    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1647.15it/s]\n",
      "1821it [01:12, 24.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "widelycritically passionatelyfirmly                                                                                                                   0.8248    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a888b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1337.15it/s]\n",
      "1821it [01:10, 25.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠwidelyĠcriticallyĠjointly Ġgenuinely                                                                                                                 0.9297    1821      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates = [\n",
    "    \"ĠwidelyĠcriticallyĠjointly Ġgenuinely\"\n",
    "]\n",
    "\n",
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e7610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1409.20it/s]\n",
      "1821it [01:10, 25.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠgenuinelyĠunequivocallyĠstrongly                                                                                                                     0.9319    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "candidates = [\n",
    "    \"ĠgenuinelyĠunequivocallyĠstrongly \"\n",
    "]\n",
    "\n",
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1495fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1741.99it/s]\n",
      "1821it [01:11, 25.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely VERY absolute VERY absolute                                                                                                                0.9325    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "candidates = [\n",
    "    \"Absolutely VERY absolute VERY absolute\"\n",
    "]\n",
    "\n",
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd8f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1746.29it/s]\n",
      "1821it [01:10, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I find very                                                                                                                                           0.8699    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1821it [00:01, 1786.11it/s]\n",
      "1821it [01:11, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I find really                                                                                                                                         0.8402    1821      \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "candidates = [\n",
    "    \"I find very\",\n",
    "    \"I find really\"\n",
    "]\n",
    "\n",
    "for candidate_prompt in candidates:\n",
    "    score = get_score(dataset_name,dataset,full_data_len,candidate_prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1988af9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
